{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import umap\n",
    "from torchvision import transforms\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler, TensorDataset\n",
    "import torch.utils.data as Data\n",
    "from visualization import draw_scores\n",
    "from preprocessing import load_data, transfer_unit_float, normalize_channel, get_training_validation_samplers, get_priors, resize, get_transitions, serialize_object, read_pickled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training will run on __ cpu __\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('The training will run on __ {} __'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## load data ##########\n",
    "mnist05_X_train, mnist05_Y_train, mnist05_X_test, mnist05_Y_test = load_data('FashionMNIST0.5.npz')\n",
    "mnist06_X_train, mnist06_Y_train, mnist06_X_test, mnist06_Y_test = load_data('FashionMNIST0.6.npz')\n",
    "cifar_X_train, cifar_Y_train, cifar_X_test, cifar_Y_test = load_data('CIFAR.npz')\n",
    "\n",
    "\n",
    "########## Transfer data type and resize ##########\n",
    "mnist05_X_train = resize(transfer_unit_float(mnist05_X_train))\n",
    "mnist05_X_test = resize(transfer_unit_float(mnist05_X_test))\n",
    "mnist06_X_train = resize(transfer_unit_float(mnist06_X_train))\n",
    "mnist06_X_test = resize(transfer_unit_float(mnist06_X_test))\n",
    "cifar_X_train = resize(transfer_unit_float(cifar_X_train), data_name=\"cifar\")\n",
    "cifar_X_test = resize(transfer_unit_float(cifar_X_test), data_name=\"cifar\")\n",
    "\n",
    "\n",
    "########## Preperation for train ##########\n",
    "mnist05_train_set = TensorDataset(torch.from_numpy(mnist05_X_train), torch.from_numpy(mnist05_Y_train))\n",
    "mnist05_test_set = TensorDataset(torch.from_numpy(mnist05_X_test), torch.from_numpy(mnist05_Y_test))\n",
    "\n",
    "mnist06_train_set = TensorDataset(torch.from_numpy(mnist06_X_train), torch.from_numpy(mnist06_Y_train))\n",
    "mnist06_test_set = TensorDataset(torch.from_numpy(mnist06_X_test), torch.from_numpy(mnist06_Y_test))\n",
    "\n",
    "cifar_train_set = TensorDataset(torch.from_numpy(cifar_X_train), torch.from_numpy(cifar_Y_train))\n",
    "cifar_test_set = TensorDataset(torch.from_numpy(cifar_X_test), torch.from_numpy(cifar_Y_test))\n",
    "\n",
    "\n",
    "\n",
    "########## Split into trainning and validation ##########\n",
    "\n",
    "mnist05_train_sampler, mnist05_validation_sampler = get_training_validation_samplers(mnist05_train_set)\n",
    "mnist06_train_sampler, mnist06_validation_sampler = get_training_validation_samplers(mnist06_train_set)\n",
    "cifar_train_sampler, cifar_validation_sampler = get_training_validation_samplers(cifar_train_set)\n",
    "\n",
    "\n",
    "########## Get transitions and priors ##########\n",
    "mnist05_transitions = get_transitions(data_name=\"mnist05\")\n",
    "mnist06_transitions = get_transitions(data_name=\"mnist06\")\n",
    "\n",
    "mnist05_priors = get_priors(mnist05_Y_test)\n",
    "mnist05_noisy_priors = get_priors(mnist05_Y_train)\n",
    "\n",
    "mnist06_priors = get_priors(mnist06_Y_test)\n",
    "mnist06_noisy_priors = get_priors(mnist06_Y_train)\n",
    "\n",
    "cifar_priors = get_priors(cifar_Y_test)\n",
    "cifar_noisy_priors = get_priors(cifar_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Different classifers: CNN, LSTM##########\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_classes, channel=1):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(channel, 16, kernel_size=5, stride=1, padding=2),#1.in_channel=input height, 2.out_channel=n_filters=n_feature_map（扫描框个数）, 3.kernal_size(扫描框大小), 4.stride=1（扫描框步长）, 5.padding（图片周围填充几轮）;  28=(32-5+1)\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          #框边长为2，每2个结果输出1个，28/2 = 14，压缩后image为14*14，高度变成了16\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          #框边长为2，每2个结果输出1个，14/2 = 7，压缩后image为7*7，高度变成了32  \n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, number_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class CNN_BN(nn.Module):\n",
    "    \n",
    "    def __init__(self, number_classes, channel=1):\n",
    "        super(CNN_BN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(channel, 16, kernel_size=5, stride=1, padding=2),#1.in_channel=input height, 2.out_channel=n_filters=n_feature_map（扫描框个数）, 3.kernal_size(扫描框大小), 4.stride=1（扫描框步长）, 5.padding（图片周围填充几轮）;  28=(32-5+1)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          #框边长为2，每2个结果输出1个，28/2 = 14，压缩后image为14*14，高度变成了16\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),          #框边长为2，每2个结果输出1个，14/2 = 7，压缩后image为7*7，高度变成了32  \n",
    "        )\n",
    "        self.out = nn.Linear(32 * 7 * 7, number_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "class CNN2(nn.Module): ## with Normalization on Batch\n",
    "\n",
    "    def __init__(self, number_classes=10, channel=3):\n",
    "        super(CNN2, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 32, kernel_size=5, stride=1, padding=0), # 28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2), # 14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # 14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2), # 7\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0), # 5\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(5*5*128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, number_classes),\n",
    "        ) # No softmax because CrossEntropy loss does it.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 128 * 5 * 5)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class CNN2_BN(nn.Module): ## with no Normalization on Batch\n",
    "\n",
    "    def __init__(self, number_classes=10):\n",
    "        super(CNN2_BN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=0), # 28\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2), # 14\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), # 14\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2), # 7\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=0), # 5\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(5*5*128, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(1024, number_classes),\n",
    "        ) # No softmax because CrossEntropy loss does it.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 128 * 5 * 5)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "# batch_size*num_features*height*width\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, number_classes=3):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.f1 = nn.LSTM(input_size=28, hidden_size=64, num_layers=1, batch_first=False)\n",
    "        self.f2 = nn.Linear(64, number_classes)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.f1(x.reshape(-1, 28, 28), None)\n",
    "        out = self.f2(r_out[:, -1, :])#忘加f2\n",
    "        return out\n",
    "    \n",
    "class LSTM_BN(nn.Module):\n",
    "    def __init__(self, number_classes=3):\n",
    "        super(LSTM_BN, self).__init__()\n",
    "        self.f1 = nn.LSTM(input_size=28, hidden_size=64, num_layers=1, batch_first=False)\n",
    "        self.f2 = nn.BatchNorm1d(28)\n",
    "        self.f3 = nn.Linear(64, number_classes)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.f1(x.reshape(-1, 28, 28), None)\n",
    "        r_out = self.f2(r_out)\n",
    "        out = self.f3(r_out[:, -1, :])#忘加f2\n",
    "        return out\n",
    "    \n",
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, number_classes=3):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.f1 = nn.LSTM(input_size=32 * 3, hidden_size=64, num_layers=3, batch_first=False)\n",
    "        self.f2 = nn.Linear(64, number_classes)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.f1(x.reshape(-1, 32, 32*3), None)\n",
    "        out = self.f2(r_out[:, -1, :])#忘加f2\n",
    "        return out\n",
    "    \n",
    "class LSTM2_BN(nn.Module):\n",
    "    def __init__(self, number_classes=3):\n",
    "        super(LSTM2_BN, self).__init__()\n",
    "        self.f1 = nn.LSTM(input_size=32 * 3, hidden_size=64, num_layers=3, batch_first=False)\n",
    "        self.f2 = nn.BatchNorm1d(32)\n",
    "        self.f3 = nn.Linear(64, number_classes)\n",
    "    def forward(self, x):\n",
    "        r_out, (h_n, h_c) = self.f1(x.reshape(-1, 32, 32*3), None)\n",
    "        r_out = self.f2(r_out)\n",
    "        out = self.f3(r_out[:, -1, :])#忘加f2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "class Evaluation(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "\n",
    "    \n",
    "class Recordings():\n",
    "    \n",
    "    def __init__(self, progress_recording_frequency, evaluation_recording_frequency):\n",
    "        self.progress = []\n",
    "        self.evaluation = Evaluation()\n",
    "        self.progress_recording_frequency = progress_recording_frequency\n",
    "        self.evaluation_recording_frequency = evaluation_recording_frequency\n",
    "    \n",
    "    def log_progress(self, epoch, epochs, iteration, iterations):\n",
    "        print(f'\\n---- Epoch {epoch + 1}/{epochs}, Iteration {iteration + 1}/{iterations}')\n",
    "        print('Loss : {:.4f}'.format(self.progress[-1]))\n",
    "\n",
    "    def log_evaluation(self, epoch, epochs):\n",
    "        print(f'\\n----- Recording of Evaluation for epoch {epoch + 1}/{epochs}')\n",
    "        print('Loss: {:.4f}, Accuracy: {:.4f}'.format(self.evaluation.loss[-1], self.evaluation.accuracy[-1]))\n",
    "        \n",
    "    def display(self):\n",
    "        iterations_axes = [i * self.progress_recording_frequency for i in range(len(self.progress))]\n",
    "\n",
    "        # Progress Loss\n",
    "        fig, (ax1) = plt.subplots(1, 1, figsize=(9, 2))\n",
    "        ax1.plot(iterations_axes, self.progress, color='darkslategray')\n",
    "        fig.suptitle('Progress Loss')\n",
    "        ax1.set(xlabel='Iterations', ylabel='Loss')\n",
    "        plt.show()\n",
    "\n",
    "        iterations_axes = [i * self.evaluation_recording_frequency for i in range(len(self.evaluation.loss))]\n",
    "\n",
    "        # Evaluation Metrics\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 2))\n",
    "        ax1.plot(iterations_axes, self.evaluation.loss, color='darkslategray')\n",
    "        ax1.set(xlabel='Epochs', ylabel='Loss')\n",
    "        ax2.plot(iterations_axes, self.evaluation.accuracy, color='darkslategray')\n",
    "        ax2.set(xlabel='Epochs', ylabel='Accuracy')\n",
    "        fig.suptitle('Evaluation Metrics Recording')\n",
    "        plt.show()\n",
    "        \n",
    "    def show_loss_rmse(self):\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 2))\n",
    "        loss_his = self.evaluation.loss\n",
    "        rmse_his = self.evaluation.accuracy\n",
    "        x_loss = range(len(loss_his))\n",
    "        ax1.plot(x_loss, loss_his, label='loss', color = 'b', linewidth=3)\n",
    "        ax1.set_xlabel('epoches')\n",
    "        ax1.set_ylabel('loss')\n",
    "        ax1.legend()\n",
    "\n",
    "        x_rmse = range(len(rmse_his))\n",
    "        ax2.plot(x_rmse, rmse_his, label='rmse', color = 'r', linewidth=3)\n",
    "        ax2.set_xlabel('epoches')\n",
    "        ax2.set_ylabel('rmse')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "def evaluate(model, loader, device, validation=False):\n",
    "    # Eval flag\n",
    "    model.eval()\n",
    "\n",
    "    # Initialize metrics\n",
    "    loss = 0.\n",
    "    accuracy = 0.\n",
    "    \n",
    "    # Criterion\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in loader:\n",
    "            # To device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Prediction\n",
    "            label_outputs = model(inputs.float())\n",
    "\n",
    "            # Update loss\n",
    "            loss += criterion(label_outputs, labels).data.item()\n",
    "\n",
    "            # Update Accuracies\n",
    "            predicted_labels = label_outputs.data.max(1)[1]\n",
    "            accuracy += predicted_labels.eq(labels.data).sum().item()\n",
    "\n",
    "    # Average metrics\n",
    "    loss /= len(loader)\n",
    "    if validation:        \n",
    "        accuracy /= len(loader.sampler.indices)\n",
    "    else:\n",
    "        accuracy /= len(loader.dataset)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, dataset, device, params, test_loader, transition_matrix=None, verbose=False):\n",
    "    print(\"Training by 10-fold and repeat with 10 epoches.....\")\n",
    "    if verbose:\n",
    "        print('==== Start Training ====')\n",
    "    # Train Flag\n",
    "    model.train()\n",
    "    \n",
    "    # Recordings\n",
    "    recordings = Recordings(params.log_progress_every, params.evaluate_model_every)\n",
    "\n",
    "    # Number of classes\n",
    "    nbr_classes = len(set(dataset.tensors[1].numpy()))\n",
    "\n",
    "    # Type casting for inverse transition matrix\n",
    "    if transition_matrix is not None:\n",
    "        transition_matrix = transition_matrix.float()\n",
    "    \n",
    "    for epoch in range(params.epochs):\n",
    "\n",
    "        # For shuffling, regenerate the dataloaders\n",
    "        t_sampler, v_sampler = get_training_validation_samplers(dataset)\n",
    "        t_loader = DataLoader(dataset, batch_size=params.batch_size, sampler=t_sampler)\n",
    "        v_loader = DataLoader(dataset, batch_size=params.batch_size, sampler=v_sampler)\n",
    "        \n",
    "        for iteration, (inputs, labels) in enumerate(t_loader):\n",
    "            # Prepare to device\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Reset grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Outputs\n",
    "            output_labels = model(inputs.float())\n",
    "\n",
    "            # Loss\n",
    "            if transition_matrix is not None:\n",
    "                unique_labels = [c * torch.ones(labels.size()[0]).long().to(device) for c in range(nbr_classes)]\n",
    "                losses = [criterion(output_labels, l) for l in unique_labels]\n",
    "                losses = torch.stack(losses)\n",
    "                corrected_losses = losses.transpose(0,1)@transition_matrix\n",
    "                loss = corrected_losses.gather(1, labels.view(-1,1)).mean()\n",
    "            else:\n",
    "                loss = criterion(output_labels, labels).mean()\n",
    "\n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # =======\n",
    "            # Log Progress\n",
    "            if ((iteration + 1) % params.log_progress_every == 0):\n",
    "                recordings.progress.append(loss.data.item())\n",
    "                if verbose:\n",
    "                    recordings.log_progress(epoch, params.epochs, iteration, len(t_loader))\n",
    "        \n",
    "        # Evaluate\n",
    "        if ((epoch + 1) % params.evaluate_model_every == 0):\n",
    "            evaluation_loss, evaluation_accuracy = evaluate(model, test_loader, device, validation=False)\n",
    "            recordings.evaluation.loss.append(evaluation_loss); recordings.evaluation.accuracy.append(evaluation_accuracy)\n",
    "\n",
    "            if verbose:\n",
    "                recordings.log_evaluation(epoch, params.epochs)\n",
    "\n",
    "        # Save temporary model\n",
    "        if ((epoch + 1) % params.save_model_every == 0):\n",
    "            torch.save(model.state_dict(), params.model_filename)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model.state_dict(), params.model_filename)\n",
    "\n",
    "    # Save recordings\n",
    "    serialize_object(recordings, params.recordings_filename)\n",
    "\n",
    "    if verbose:\n",
    "        print('==== End Training ====')\n",
    "    \n",
    "    return model, recordings\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    batch_size = 252\n",
    "    epochs = 10\n",
    "    learning_rate = 0.01\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0\n",
    "    \n",
    "    log_progress_every = 50 # in iterations\n",
    "    evaluate_model_every = 1 # in epochs\n",
    "\n",
    "    save_model_every = 25 # in epochs\n",
    "    model_filename = '{}/saves/model.pth'.format('.')\n",
    "    recordings_filename = '{}/saves/recordings.pickle'.format('.')\n",
    "\n",
    "def select_train(data_name='mnist05', classifer=\"cnn\", optimizer_name='sgd', loss='crossentropy', estimation=True):\n",
    "    params = Params()\n",
    "    if classifer == 'cnn':\n",
    "        model = CNN(number_classes=3)\n",
    "    elif classifer == 'cnn-bn':\n",
    "        model = CNN_BN(number_classes=3)\n",
    "    elif classifer == 'cnn2':\n",
    "        model = CNN2(number_classes=3)\n",
    "    elif classifer == 'cnn2-bn':\n",
    "        model = CNN2_BN(number_classes=3)\n",
    "    elif classifer == 'lstm':\n",
    "        model = LSTM(number_classes=3)\n",
    "    elif classifer == 'lstm-bn':\n",
    "        model = LSTM_BN(number_classes=3)\n",
    "    elif classifer == 'lstm2':\n",
    "        model = LSTM2(number_classes=3) \n",
    "    elif classifer == 'lstm2-bn':\n",
    "        model = LSTM2_BN(number_classes=3) \n",
    "    else:\n",
    "        print('Wrong name of classifer.')\n",
    "        return\n",
    "    \n",
    "    if optimizer_name == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=params.learning_rate, momentum=params.momentum, weight_decay=params.weight_decay)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=params.learning_rate, momentum=params.momentum, weight_decay=params.weight_decay)\n",
    "    elif optimizer_name == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=params.learning_rate, momentum=params.momentum, weight_decay=params.weight_decay)\n",
    "    else:\n",
    "        print(\"Wrong name of optimizer.\")\n",
    "        return\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')   \n",
    "    \n",
    "    ########## Wrap data with loader ##########\n",
    "    mnist05_train_loader = DataLoader(mnist05_train_set, batch_size=params.batch_size, sampler=mnist05_train_sampler)\n",
    "    mnist05_validation_loader = DataLoader(mnist05_train_set, batch_size=params.batch_size, sampler=mnist05_validation_sampler)\n",
    "    mnist05_test_loader = DataLoader(mnist05_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "    mnist06_train_loader = DataLoader(mnist06_train_set, batch_size=params.batch_size, sampler=mnist06_train_sampler)\n",
    "    mnist06_validation_loader = DataLoader(mnist06_train_set, batch_size=params.batch_size, sampler=mnist06_validation_sampler)\n",
    "    mnist06_test_loader = DataLoader(mnist06_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "    cifar_train_loader = DataLoader(cifar_train_set, batch_size=params.batch_size, sampler=cifar_train_sampler)\n",
    "    cifar_validation_loader = DataLoader(cifar_train_set, batch_size=params.batch_size, sampler=cifar_validation_sampler)\n",
    "    cifar_test_loader = DataLoader(cifar_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # ====================================\n",
    "    # Choose dataset and Transition Matrix\n",
    "    datasets_dict = {\n",
    "        \"mnist05\": {\n",
    "            \"loaders\": (mnist05_train_loader, mnist05_validation_loader, mnist05_test_loader),\n",
    "            \"dataset\": mnist05_train_set,\n",
    "            \"testset\": mnist05_test_set,\n",
    "            \"transition_matrix\": torch.from_numpy(mnist05_transitions.T).to(device) # Transpose because defined differently than in lectures\n",
    "            },\n",
    "        \"mnist06\": {\n",
    "            \"loaders\": (mnist06_train_loader, mnist06_validation_loader, mnist06_test_loader),\n",
    "            \"dataset\": mnist06_train_set,\n",
    "            \"testset\": mnist06_test_set,\n",
    "            \"transition_matrix\": torch.from_numpy(mnist06_transitions.T).to(device) # Transpose because defined differently than in lectures\n",
    "            },\n",
    "        \"cifar\": {\n",
    "            \"loaders\": (cifar_train_loader, cifar_validation_loader, cifar_test_loader),\n",
    "            \"dataset\": cifar_train_set,\n",
    "            \"testset\": cifar_test_set,\n",
    "            \"transition_matrix\": torch.from_numpy(np.array(\n",
    "                [[0.2796548,  0.37073188, 0.33353805],\n",
    "                [0.37033914, 0.35247205, 0.28679701],\n",
    "                [0.35000606, 0.27679607, 0.37966494]]\n",
    "                ).T).to(device)\n",
    "            },\n",
    "    }\n",
    "\n",
    "\n",
    "    train_loader, validation_loader, test_loader = datasets_dict[data_name]['loaders']\n",
    "    dataset = datasets_dict[data_name]['dataset']\n",
    "    testset = datasets_dict[data_name]['testset']\n",
    "\n",
    "    if not estimation:\n",
    "        transition_matrix = datasets_dict[data_name]['transition_matrix']\n",
    "    else:\n",
    "        transition_matrix = None\n",
    "    \n",
    "    model, recordings = train(model, optimizer, criterion, dataset, device, params, test_loader, transition_matrix=transition_matrix)\n",
    "    #loss, accuracy = evaluate(model, test_loader, device) #\n",
    "    return recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    " def draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"\", file=\"\"):\n",
    "    \n",
    "    labels = ['CNN', 'CNN-BN', 'LSTM', 'LSTM-BN']\n",
    "\n",
    "    x = np.arange(len(labels))  # the label locations\n",
    "    y = np.arange(0, 1, 0.1)\n",
    "    width = 0.25  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))#可调整柱子粗细高低\n",
    "    rects1 = ax.bar(x - width, mean1, width, alpha=0.7, color='r' , yerr=std1, label='mnist05')\n",
    "    rects2 = ax.bar(x, mean2, width, alpha=0.7, color='b', yerr=std2, label='mnist06')\n",
    "    rects3 = ax.bar(x + width, mean3, width, alpha=0.7, color='g', yerr=std3, label='cifar')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.legend(loc=(0.65, 0.9))\n",
    "    \n",
    "    for rect in rects1:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.3f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "    for rect in rects2:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.3f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "    for rect in rects3:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{:.3f}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(file + \".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Impact of classifers with Transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Different classifers(cnn, cnn-bn, lstm, lstm-bn) by sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1, mean2, mean3 = [], [], []\n",
    "std1, std2, std3 = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 with T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn: | Mean of Accuracy: 0.502133 | Std of Accuracy: 0.038739 | Max of Accuracy: 0.535333 | Min of Accuracy: 0.394667\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn-bn: | Mean of Accuracy: 0.820500 | Std of Accuracy: 0.082232 | Max of Accuracy: 0.892667 | Min of Accuracy: 0.617333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm: | Mean of Accuracy: 0.724033 | Std of Accuracy: 0.035166 | Max of Accuracy: 0.755667 | Min of Accuracy: 0.633333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm-bn: | Mean of Accuracy: 0.714367 | Std of Accuracy: 0.026912 | Max of Accuracy: 0.742333 | Min of Accuracy: 0.660667\n"
     ]
    }
   ],
   "source": [
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn: | Mean of Accuracy: 0.333333 | Std of Accuracy: 0.000000 | Max of Accuracy: 0.333333 | Min of Accuracy: 0.333333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn-bn: | Mean of Accuracy: 0.531900 | Std of Accuracy: 0.108761 | Max of Accuracy: 0.679000 | Min of Accuracy: 0.333667\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm: | Mean of Accuracy: 0.439833 | Std of Accuracy: 0.032891 | Max of Accuracy: 0.480667 | Min of Accuracy: 0.377000\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm-bn: | Mean of Accuracy: 0.482400 | Std of Accuracy: 0.057481 | Max of Accuracy: 0.547667 | Min of Accuracy: 0.357000\n"
     ]
    }
   ],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn2: | Mean of Accuracy: 0.319900 | Std of Accuracy: 0.012831 | Max of Accuracy: 0.351667 | Min of Accuracy: 0.302333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn2-bn: | Mean of Accuracy: 0.332900 | Std of Accuracy: 0.021520 | Max of Accuracy: 0.393333 | Min of Accuracy: 0.305667\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm2: | Mean of Accuracy: 0.326667 | Std of Accuracy: 0.003169 | Max of Accuracy: 0.330667 | Min of Accuracy: 0.320667\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm2-bn: | Mean of Accuracy: 0.334467 | Std of Accuracy: 0.007242 | Max of Accuracy: 0.344333 | Min of Accuracy: 0.321667\n"
     ]
    }
   ],
   "source": [
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers with transition matrix by SGD\", file=\"classifers_with_transition_sgd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 without T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn: | Mean of Accuracy: 0.666667 | Std of Accuracy: 0.056024 | Max of Accuracy: 0.720667 | Min of Accuracy: 0.542333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn-bn: | Mean of Accuracy: 0.883533 | Std of Accuracy: 0.095701 | Max of Accuracy: 0.930333 | Min of Accuracy: 0.598667\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm: | Mean of Accuracy: 0.743500 | Std of Accuracy: 0.017149 | Max of Accuracy: 0.771333 | Min of Accuracy: 0.713333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "lstm-bn: | Mean of Accuracy: 0.742467 | Std of Accuracy: 0.008265 | Max of Accuracy: 0.754667 | Min of Accuracy: 0.725333\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "cnn: | Mean of Accuracy: 0.412167 | Std of Accuracy: 0.017428 | Max of Accuracy: 0.419667 | Min of Accuracy: 0.360000\n",
      "Training by 10-fold and repeat with 10 epoches.....\n"
     ]
    }
   ],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='sgd', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers with no transition matrix by SGD\", file=\"classifers_with_no_transition_sgd\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Different classifers(cnn, cnn-bn, lstm, lstm-bn) by RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1, mean2, mean3 = [], [], []\n",
    "std1, std2, std3 = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 with T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers with transition matrix RMSprop\", file=\"classifers_with_transition_RMS\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 without T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='rmsprop', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers without transition matrix RMSprop\", file=\"classifers_no_transition_RMS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Different classifers(cnn, cnn-bn, lstm, lstm-bn) by Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean1, mean2, mean3 = [], [], []\n",
    "std1, std2, std3 = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 with T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=False)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers with transition matrix Adam\", file=\"classifers_with_transition_adam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 without T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist05', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean1 = [np.mean(accs) for accs in acc_list]\n",
    "std1 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn', 'cnn-bn', 'lstm', 'lstm-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='mnist06', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean2 = [np.mean(accs) for accs in acc_list]\n",
    "std2 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "\n",
    "classifer_dict = ['cnn2', 'cnn2-bn', 'lstm2', 'lstm2-bn']\n",
    "acc_list = [] # [0]: cnn, [1]: cnn-bn, [2]: lstm, [4]: lstm-bn\n",
    "for i, classifer in enumerate(classifer_dict):\n",
    "    recordings = select_train(data_name='cifar', classifer=classifer, optimizer_name='adam', loss='crossentropy', estimation=True)\n",
    "    acc_list.append(recordings.evaluation.accuracy)\n",
    "    print(classifer + ': | Mean of Accuracy: {:4.6f} | Std of Accuracy: {:4.6f} | Max of Accuracy: {:4.6f} | Min of Accuracy: {:4.6f}'\\\n",
    "      .format(np.mean(acc_list[i]), np.std(acc_list[i]), \\\n",
    "              np.max(acc_list[i]), np.min(acc_list[i])))\n",
    "mean3 = [np.mean(accs) for accs in acc_list]\n",
    "std3 = [np.std(stds) for stds in acc_list]\n",
    "\n",
    "draw_scores(mean1, mean2, mean3, std1, std2, std3, title=\"classifers without transition matrix Adam\", file=\"classifers_no_transition_adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For MNIST 0.5 \n",
    "\n",
    "Benchmark accuracy on test dataset (crossvalidated) (10 epochs):  0.892667 (std 0.082232)\n",
    "\n",
    "Accuracy Estimations:\n",
    "\n",
    "\n",
    "\n",
    "| optimizer | No T | Given T | \n",
    "| ----------| ---- | ------- | \n",
    "|   sgd     |   0.8627 (0.0440)   |   0.7263 (0.1993)      | \n",
    "|   RMSprop    |   0.9313 (0.0049)   |   0.9140 (0.0123)      |             |\n",
    "|   Adam   |   0.6211 (0.0315)   |   0.9029 (0.0146)      |             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "Training by 10-fold and repeat with 10 epoches.....\n",
      "========\n",
      "Estimated Transition Matrix:\n",
      "[[0.58491454 0.16692027 0.2481652 ]\n",
      " [0.20071659 0.55103179 0.24825158]\n",
      " [0.22351401 0.18679138 0.5896946 ]]\n",
      "--------\n",
      "Standard Deviation:\n",
      "[[0.04851773 0.03867952 0.05788719]\n",
      " [0.07455217 0.17647435 0.12320584]\n",
      " [0.06653687 0.05194869 0.09505719]]\n",
      "========\n"
     ]
    }
   ],
   "source": [
    "params = Params()\n",
    "########## Wrap data with loader ##########\n",
    "mnist05_train_loader = DataLoader(mnist05_train_set, batch_size=params.batch_size, sampler=mnist05_train_sampler)\n",
    "mnist05_validation_loader = DataLoader(mnist05_train_set, batch_size=params.batch_size, sampler=mnist05_validation_sampler)\n",
    "mnist05_test_loader = DataLoader(mnist05_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "mnist06_train_loader = DataLoader(mnist06_train_set, batch_size=params.batch_size, sampler=mnist06_train_sampler)\n",
    "mnist06_validation_loader = DataLoader(mnist06_train_set, batch_size=params.batch_size, sampler=mnist06_validation_sampler)\n",
    "mnist06_test_loader = DataLoader(mnist06_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "cifar_train_loader = DataLoader(cifar_train_set, batch_size=params.batch_size, sampler=cifar_train_sampler)\n",
    "cifar_validation_loader = DataLoader(cifar_train_set, batch_size=params.batch_size, sampler=cifar_validation_sampler)\n",
    "cifar_test_loader = DataLoader(cifar_test_set, batch_size=params.batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Choose dataset and Transition Matrix\n",
    "datasets_dict = {\n",
    "    \"mnist05\": {\n",
    "        \"loaders\": (mnist05_train_loader, mnist05_validation_loader, mnist05_test_loader),\n",
    "        \"dataset\": mnist05_train_set,\n",
    "        \"testset\": mnist05_test_set,\n",
    "        \"transition_matrix\": torch.from_numpy(mnist05_transitions.T).to(device) # Transpose because defined differently than in lectures\n",
    "        },\n",
    "    \"mnist06\": {\n",
    "        \"loaders\": (mnist06_train_loader, mnist06_validation_loader, mnist06_test_loader),\n",
    "        \"dataset\": mnist06_train_set,\n",
    "        \"testset\": mnist06_test_set,\n",
    "        \"transition_matrix\": torch.from_numpy(mnist06_transitions.T).to(device) # Transpose because defined differently than in lectures\n",
    "        },\n",
    "    \"cifar\": {\n",
    "        \"loaders\": (cifar_train_loader, cifar_validation_loader, cifar_test_loader),\n",
    "        \"dataset\": cifar_train_set,\n",
    "        \"testset\": cifar_test_set,\n",
    "        \"transition_matrix\": torch.from_numpy(np.array(\n",
    "            [[0.2796548,  0.37073188, 0.33353805],\n",
    "            [0.37033914, 0.35247205, 0.28679701],\n",
    "            [0.35000606, 0.27679607, 0.37966494]]\n",
    "            ).T).to(device)\n",
    "        },\n",
    "}\n",
    "# ==============\n",
    "# Change dataset here\n",
    "data_name = 'cifar'\n",
    "# Change whether to train the network for transition matrix estimation\n",
    "ESTIMATION = True\n",
    "# ==============\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not ESTIMATION:\n",
    "    transition_matrix = datasets_dict[DATASET]['transition_matrix']\n",
    "else:\n",
    "    transition_matrix = None\n",
    "\n",
    "train_loader, validation_loader, test_loader = datasets_dict[data_name]['loaders']\n",
    "dataset = datasets_dict[data_name]['dataset']\n",
    "testset = datasets_dict[data_name]['testset']\n",
    "\n",
    "# Note that the transition matrix estimated is actually the transpose of the transition matrix defined in report.\n",
    "def estimate_transition_matrix(trained_model, train_set, test_set, device):\n",
    "    \n",
    "    # First, find the anchor point xi\n",
    "    nbr_classes = len(set(train_set.tensors[1].numpy()))\n",
    "    nbr_train_samples = len((train_set.tensors[1].numpy()))\n",
    "\n",
    "    x_i = []\n",
    "    softmax = nn.Softmax(dim=1) # Softmax needed as it is not included in the architecture\n",
    "    with torch.no_grad():\n",
    "        trained_model.eval()\n",
    "        all_outputs_train = trained_model(train_set.tensors[0].float().to(device)).cpu().numpy()\n",
    "        all_outputs_test = trained_model(test_set.tensors[0].float().to(device)).cpu().numpy()\n",
    "\n",
    "    # Merge train set and test set to get a bigger set to draw our samples from\n",
    "    all_outputs = np.concatenate((all_outputs_train, all_outputs_test))\n",
    "    x_i_indices = np.argmax(all_outputs, axis=0)\n",
    "\n",
    "    estimated_transition_matrix = np.zeros((nbr_classes,nbr_classes))\n",
    "    with torch.no_grad():\n",
    "        trained_model.eval()\n",
    "        for i, index in enumerate(x_i_indices):\n",
    "            if index < nbr_train_samples:\n",
    "                p_i = softmax(trained_model(train_set.tensors[0][index].float().to(device).view(1, 3, 32, 32)))\n",
    "            else:\n",
    "                test_index = index - nbr_train_samples\n",
    "                p_i = softmax(trained_model(train_set.tensors[0][test_index].float().to(device).view(1, 3, 32, 32)))\n",
    "            estimated_transition_matrix[i,:] = p_i.cpu().numpy()[0]\n",
    "\n",
    "    return estimated_transition_matrix\n",
    "\n",
    "# Generalization for estimation of the transtion matrix\n",
    "\n",
    "N = 5\n",
    "ESTIMATION = True\n",
    "params.epochs = 7\n",
    "\n",
    "nbr_classes = len(set(dataset.tensors[1].numpy()))\n",
    "estimates = np.zeros((N, nbr_classes, nbr_classes))\n",
    "for n in range(N):\n",
    "    model = CNN2(number_classes=nbr_classes).to(device).float()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params.learning_rate, momentum=params.momentum, weight_decay=params.weight_decay)\n",
    "    trained_model, _ = train(model, optimizer, criterion, dataset, device, params, test_loader, transition_matrix=transition_matrix, verbose=False)\n",
    "    estimates[n,:,:] = estimate_transition_matrix(trained_model, dataset, testset, device)\n",
    "\n",
    "print('========\\nEstimated Transition Matrix:\\n{}\\n--------\\nStandard Deviation:\\n{}\\n========'.format(\n",
    "    np.mean(estimates, axis=0), np.std(estimates, axis=0)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32 * 7 * 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
